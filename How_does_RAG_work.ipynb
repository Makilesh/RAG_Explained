{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8d65d1",
   "metadata": {},
   "source": [
    "# Step 1: Document Ingestion\n",
    "### What It Does and Why It Matters\n",
    "This is where you feed your \"knowledge base\" into the system—think loading books onto shelves. It matters because without good docs, RAG can't retrieve anything useful. Analogy: Like stocking a fridge before cooking; empty fridge = no meal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417acff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elephants use their trunks for smelling, breathing, trumpeting, drinking, and grabbing things.\n",
      "The A\n"
     ]
    }
   ],
   "source": [
    "# Import the tool we need\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load a text file\n",
    "loader = TextLoader(\"knowledge.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Print to check\n",
    "print(documents[0].page_content[:100])  # Shows first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e88d0",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. from langchain.document_loaders import TextLoader: Brings in a helper from LangChain (a library that makes AI easier).\n",
    "2. loader = TextLoader(\"knowledge.txt\"): Points to your text file.\n",
    "3. documents = loader.load(): Reads the file into a list.\n",
    "4. print(...): Just to peek at what's loaded.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: File not found? Check the path—use full path like /Users/you/docs/your_file.txt.\n",
    "* Tip: Start with a small .txt file. If it's a PDF, use PyPDFLoader instead (install pypdf via pip).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356d017",
   "metadata": {},
   "source": [
    "# Step 2: Embedding Creation\n",
    "### What It Does and Why It Matters\n",
    "Embeddings are like turning words into math coordinates (vectors) so AI can measure \"similarity\"—e.g., \"cat\" and \"kitten\" are close. Why? Searching text directly is slow; vectors are fast. Analogy: Like assigning GPS coords to places; similar spots are nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f44f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "# Import the embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Pick a simple model (free from Hugging Face)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Your text chunks (from ingestion)\n",
    "texts = [\"Cats are furry animals.\", \"Dogs love to play fetch.\"]\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Print shape to check\n",
    "print(embeddings.shape)  # Should be (2, 384) for two texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a608a5",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. from sentence_transformers import SentenceTransformer: Gets the tool.\n",
    "2. model = SentenceTransformer('all-MiniLM-L6-v2'): Loads a free model—downloads automatically.\n",
    "3. texts = [...]: Your document pieces (split big docs into chunks).\n",
    "4. embeddings = model.encode(texts): Turns texts into vectors.\n",
    "5. print(embeddings.shape): Shows dimensions (number of texts x vector size).\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Model download fails? Check internet. If slow, try a smaller model like 'paraphrase-MiniLM-L3-v2'.\n",
    "* Tip: Split long docs into sentences for better embeddings—use text.split('.').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306451e",
   "metadata": {},
   "source": [
    "# Step 3: Retrieval\n",
    "\n",
    "This searches your embeddings for the best matches to a question. It's the \"retrieval\" in RAG—finding relevant info fast from thousands of documents. Why? Because searching through plain text is slow, but comparing vectors is lightning quick. Analogy: Like a librarian who's memorized where every book is and can grab the right one in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830794fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats are furry animals.\n"
     ]
    }
   ],
   "source": [
    "# Import the vector store\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Your embeddings from before (as numpy array)\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create index (like a search database)\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Query embedding\n",
    "query = \"What do cats look like?\"\n",
    "query_emb = model.encode([query])\n",
    "\n",
    "# Search (k=1 means top 1 result)\n",
    "D, I = index.search(query_emb.astype('float32'), k=1)\n",
    "\n",
    "# Get the text\n",
    "retrieved_text = texts[I[0][0]]\n",
    "print(retrieved_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321777f",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. import faiss and numpy: Tools for fast search and math.\n",
    "2. embeddings_np = ...: Convert to format FAISS likes.\n",
    "3. index = faiss.IndexFlatL2(...): Builds a simple search index.\n",
    "4. index.add(...): Adds your embeddings.\n",
    "5. query_emb = ...: Embed the question.\n",
    "6. D, I = index.search(...): Finds closest (I is indices).\n",
    "7. retrieved_text = ...: Gets the matching text.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Shape mismatch? Ensure embeddings are 2D arrays.\n",
    "* Tip: For more results, set k=3. FAISS is fast even for thousands of docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac618a28",
   "metadata": {},
   "source": [
    "# Step 4: Augmentation\n",
    "\n",
    "Combines the user's question with retrieved documents into a better prompt. Why? The AI needs context—raw questions often don't have enough info. This step adds the \"open book\" to the exam. Analogy: Like giving a chef both the order and the available ingredients before they start cooking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9b371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on this info: Cats are furry animals.\n",
      "Answer: What do cats look like?\n"
     ]
    }
   ],
   "source": [
    "# Your question\n",
    "question = \"What do cats look like?\"\n",
    "\n",
    "# Retrieved text from before\n",
    "retrieved = \"Cats are furry animals.\"\n",
    "\n",
    "# Augment: Simple template\n",
    "prompt = f\"Based on this info: {retrieved}\\nAnswer: {question}\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55a74e",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    " \n",
    "1. question = ...: The user's ask.\n",
    "2. retrieved = ...: From retrieval.\n",
    "3. prompt = f\"...\": Formats a string with both—f-string is Python magic for inserting variables.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Prompt too long? AI models have limits—keep under 2000 words.\n",
    "* Tip: Make templates fancier, like adding \"Be helpful and accurate.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7614c2",
   "metadata": {},
   "source": [
    "# Step 5: Generation\n",
    "\n",
    "Uses an AI model to create the final answer from the augmented prompt. This is where the magic happens—turning retrieved data into human-like responses. Why? Because raw data isn't helpful; users need natural language answers. Analogy: The librarian writing you a summary instead of just handing you a stack of books."
   ]
  },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on this info: Cats are furry animals.\n",
      "Answer: What do cats look like?\n",
      "Answer: The cats are the ones that are the most common in the world. Cats are the most common of all mammals, and they're also the most intelligent in the world. Cats are also the most active in the world, but they're also a part of the human species. They're also the most beautiful, and the most intelligent.\n",
      "Answer: No, cats are not the most intelligent at all. Cats are just part of the human population.\n",
      "Answer: Cats are the most important and most beautiful animals in the world.\n",
      "Answer: Cats are the most misunderstood, and they're also the most stupid.\n",
      "Answer: Humans have been known to be the most ignorant. Humans are often considered to be the most stupid animals in the world, and humans are considered to be the most stupid animals in the world.\n",
      "Answer: Humans are the only ones that can control their own behavior.\n",
      "Answer: Humans are the only humans that can control their own actions.\n",
      "Answer: Humans have a lot of special qualities.\n",
      "Answer: Humans are the only ones that control their own behaviors.\n",
      "Answer: Humans are the only people of the world that can control their own behavior.\n",
      "Answer: Humans are the only animals in the world that can control their own\n"
     ]
    }
   ],
   "source": [
    "# Import generator (using free Hugging Face)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a small model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Your augmented prompt\n",
    "prompt = \"Based on this info: Cats are furry animals.\\nAnswer: What do cats look like?\"\n",
    "\n",
    "# Generate\n",
    "response = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486b995",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. from transformers import pipeline: Gets the generation tool.\n",
    "2. generator = pipeline(...): Loads 'gpt2'—a free, small AI.\n",
    "3. prompt = ...: From augmentation.\n",
    "4. response = generator(...): Runs it (max_length limits output).\n",
    "5. print(response): Shows the answer.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Output gibberish? Try better models like 'distilgpt2' or paid ones (add OpenAI key).\n",
    "* Tip: Set max_length low for speed. Install torch if errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
