{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8d65d1",
   "metadata": {},
   "source": [
    "# Step 1: Document Ingestion\n",
    "### What It Does and Why It Matters\n",
    "This is where you feed your \"knowledge base\" into the system—think loading books onto shelves. It matters because without good docs, RAG can't retrieve anything useful. Analogy: Like stocking a fridge before cooking; empty fridge = no meal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417acff5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import the tool we need\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load a text file (replace 'knowledge.txt' with your actual file)\n",
    "loader = TextLoader(\"knowledge.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Print to check\n",
    "print(documents[0].page_content[:100])  # Shows first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e88d0",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. from langchain.document_loaders import TextLoader: Brings in a helper from LangChain (a library that makes AI easier).\n",
    "2. loader = TextLoader(\"your_file.txt\"): Points to your text file.\n",
    "3. documents = loader.load(): Reads the file into a list.\n",
    "4. print(...): Just to peek at what's loaded.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: File not found? Check the path—use full path like /Users/you/docs/your_file.txt.\n",
    "* Tip: Start with a small .txt file. If it's a PDF, use PyPDFLoader instead (install pypdf via pip).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356d017",
   "metadata": {},
   "source": [
    "# Step 2: Embedding Creation\n",
    "### What It Does and Why It Matters\n",
    "Embeddings are like turning words into math coordinates (vectors) so AI can measure \"similarity\"—e.g., \"cat\" and \"kitten\" are close. Why? Searching text directly is slow; vectors are fast. Analogy: Like assigning GPS coords to places; similar spots are nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44f887",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import the embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Pick a simple model (free from Hugging Face)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Your text chunks (from ingestion)\n",
    "texts = [\"Cats are furry animals.\", \"Dogs love to play fetch.\"]\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Print shape to check\n",
    "print(embeddings.shape)  # Should be (2, 384) for two texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a608a5",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. from sentence_transformers import SentenceTransformer: Gets the tool.\n",
    "2. model = SentenceTransformer('all-MiniLM-L6-v2'): Loads a free model—downloads automatically.\n",
    "3. texts = [...]: Your document pieces (split big docs into chunks).\n",
    "4. embeddings = model.encode(texts): Turns texts into vectors.\n",
    "5. print(embeddings.shape): Shows dimensions (number of texts x vector size).\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Model download fails? Check internet. If slow, try a smaller model like 'paraphrase-MiniLM-L3-v2'.\n",
    "* Tip: Split long docs into sentences for better embeddings—use text.split('.').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306451e",
   "metadata": {},
   "source": [
    "# Step 3: Retrieval\n",
    "\n",
    "This searches your embeddings for the best matches to a question. Why? It's the \"retrieval\" in RAG—finds relevant info quickly. Analogy: Librarian scanning shelves for book titles matching your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830794fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import the vector store\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Your embeddings from before (as numpy array)\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create index (like a search database)\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Query embedding\n",
    "query = \"What do cats look like?\"\n",
    "query_emb = model.encode([query])\n",
    "\n",
    "# Search (k=1 means top 1 result)\n",
    "D, I = index.search(query_emb.astype('float32'), k=1)\n",
    "\n",
    "# Get the text\n",
    "retrieved_text = texts[I[0][0]]\n",
    "print(retrieved_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321777f",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. import faiss and numpy: Tools for fast search and math.\n",
    "2. embeddings_np = ...: Convert to format FAISS likes.\n",
    "3. index = faiss.IndexFlatL2(...): Builds a simple search index.\n",
    "4. index.add(...): Adds your embeddings.\n",
    "5. query_emb = ...: Embed the question.\n",
    "6. D, I = index.search(...): Finds closest (I is indices).\n",
    "7. retrieved_text = ...: Gets the matching text.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Shape mismatch? Ensure embeddings are 2D arrays.\n",
    "* Tip: For more results, set k=3. FAISS is fast even for thousands of docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac618a28",
   "metadata": {},
   "source": [
    "# Step 4: Augmentation\n",
    "\n",
    "Combines the question with retrieved docs into a better prompt. Why? Helps the AI generate accurate answers. Analogy: Giving the librarian your question plus book snippets to craft a full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b371e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Your question\n",
    "question = \"What do cats look like?\"\n",
    "\n",
    "# Retrieved text from before\n",
    "retrieved = \"Cats are furry animals.\"\n",
    "\n",
    "# Augment: Simple template\n",
    "prompt = f\"Based on this info: {retrieved}\\nAnswer: {question}\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55a74e",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    " \n",
    "1. question = ...: The user's ask.\n",
    "2. retrieved = ...: From retrieval.\n",
    "3. prompt = f\"...\": Formats a string with both—f-string is Python magic for inserting variables.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Prompt too long? AI models have limits—keep under 2000 words.\n",
    "* Tip: Make templates fancier, like adding \"Be helpful and accurate.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7614c2",
   "metadata": {},
   "source": [
    "# Step 5: Generation\n",
    "\n",
    "Uses an AI model to create the final answer from the augmented prompt. Why? This is where magic happens—turning data into human-like text. Analogy: The librarian writing a summary based on fetched books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03137f26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import generator (using free Hugging Face)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a small model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Your augmented prompt\n",
    "prompt = \"Based on this info: Cats are furry animals.\\nAnswer: What do cats look like?\"\n",
    "\n",
    "# Generate\n",
    "response = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486b995",
   "metadata": {},
   "source": [
    "# Line-by-Line Walkthrough\n",
    "\n",
    "1. from transformers import pipeline: Gets the generation tool.\n",
    "2. generator = pipeline(...): Loads 'gpt2'—a free, small AI.\n",
    "3. prompt = ...: From augmentation.\n",
    "4. response = generator(...): Runs it (max_length limits output).\n",
    "5. print(response): Shows the answer.\n",
    "\n",
    "# Common Pitfalls and Tips\n",
    "\n",
    "* Pitfall: Output gibberish? Try better models like 'distilgpt2' or paid ones (add OpenAI key).\n",
    "* Tip: Set max_length low for speed. Install torch if errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
